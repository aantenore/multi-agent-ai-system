# LLM Provider Configuration
# Choose: "ollama" for local, "openai" for remote
LLM_PROVIDER=ollama

# Model to use
# Ollama: mistral, codellama, llama3.2
# OpenAI: gpt-4o, gpt-4o-mini
LLM_MODEL=mistral

# OpenAI API (only if LLM_PROVIDER=openai)
OPENAI_API_KEY=sk-your-key-here
OPENAI_BASE_URL=https://api.openai.com/v1

# Ollama (default localhost)
OLLAMA_HOST=http://localhost:11434

# Logging
LOG_LEVEL=INFO
